{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77006fa0-d390-404b-aa8e-45c456755053",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# This calculates, summarizes, exports, plots the Fiji image quantification\n",
    "Uses Fiji generated .txt files as input. Adapted to different protein names.  \n",
    "Exports results and summarized results as excel file.  \n",
    "Generates violinplots out of all single cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddead3e-3c24-4d43-a7cc-e3f87888d3f5",
   "metadata": {},
   "source": [
    "## Initial processing and plotting of individual replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc08488-9a12-49da-929a-1f2993bc5a0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" open folder and combine all txt files into one dict, which is used subsequently \"\"\"\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas as pd\n",
    "pd.options.mode.copy_on_write = True\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "rawfiles = {}\n",
    "global file_dir\n",
    "file_dir = filedialog.askdirectory(title=\"Choose folder with files\")\n",
    "#file_dir = \"C:/Users/annal/Dropbox/Quantification_ERreflux\"\n",
    "print(file_dir)\n",
    "try:\n",
    "    folder_date = re.findall(r'(\\d{6})', file_dir)[1]\n",
    "except IndexError:\n",
    "    print(\"No date\")\n",
    "\n",
    "for filename in sorted(os.listdir(file_dir)):\n",
    "    print(filename)\n",
    "    file_path = os.path.join(file_dir, filename)\n",
    "    if filename.startswith(\"._\"): \n",
    "        continue\n",
    "    elif filename.startswith(\"~$\"):\n",
    "        continue\n",
    "    file_name, file_extension = os.path.splitext(filename)\n",
    "    if file_extension == \".txt\":            \n",
    "        file_data = pd.read_csv(file_path, delimiter=\"\\t\", encoding='latin1')\n",
    "    elif file_extension == \".csv\":\n",
    "        file_data = pd.read_csv(file_path, encoding='latin1')\n",
    "    else:\n",
    "        continue\n",
    "    if \"Summary\" in file_name:\n",
    "        continue\n",
    "    rawfiles[f\"{file_name}\"] = file_data\n",
    "\n",
    "root.destroy()\n",
    "rawfiles.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26312912-95ad-4f3d-a5d1-c4efe7c759ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "\n",
    "def Filter (rawfiles):\n",
    "    \"\"\" initial processing: remove columns and combine cell & organelle measurements in one row\n",
    "    unprocessed txt files have cell and organelle measurements underneath and not assigned to each other yet\n",
    "    some cells don't have a matching organelle, column names are the same\n",
    "    fiji thresholding information is on bottom two rows and don't contain measurements\n",
    "    \"\"\"\n",
    "    files = rawfiles.copy()\n",
    "    regex = r\"(?P<POI>^.+) (?P<cond>.+)_\\d*\" #when name is: POI, condition, replicate\n",
    "    \n",
    "    global POI\n",
    "    ordered_files = {}\n",
    "    for name, data in files.items():\n",
    "        POI = re.search(regex, name).group(\"POI\")\n",
    "        cond = re.search(regex, name).group(\"cond\")\n",
    "    \n",
    "    # rename channel names automatically and manually.\n",
    "        channel_rename = {\n",
    "            \"channel 2\": POI,\n",
    "            \"channel 3\": POI,\n",
    "            \"channel 1\": \"Sec63-Scarlet\"\n",
    "        }\n",
    "    #Drop the bottom rows & other unncessary columns & rename\n",
    "        data = data.dropna()\n",
    "        proc_data = data.drop(columns=[' ','MinThr','MaxThr','Message'])\n",
    "        proc_data.columns = [re.sub(r'^Mean', 'Px Int Mean', col) for col in proc_data.columns]\n",
    "    #Generate new index based on Label numbers \"POI-sfGFP nt_002_cell channel 2:0001-0054\"\n",
    "        num_pattern = r\"^.*_(?P<org>\\w*) (?P<channel>.*):(?P<num>\\d*)-\\d*\" #match organelle, channel x and number before -\n",
    "        #go through Label and extract channels and numbers in different columns, generate new index\n",
    "        proc_data[['Organelles', 'Channel', 'Cell_No']] = proc_data['Label'].str.extract(num_pattern)\n",
    "        proc_data['Cell_No'] = proc_data['Cell_No'].astype(int)\n",
    "        proc_data[\"Channel\"] = proc_data[\"Channel\"].map(channel_rename) #rename channels with definded dict POI name\n",
    "    #Split dfs based on channel and organelle, merge the same channels \n",
    "    #aka match each invidual measured cell with its matching organelle\n",
    "        for channel in proc_data[\"Channel\"].unique():\n",
    "        # 1. Split/ Filter df by channel\n",
    "            ch_df = proc_data[proc_data[\"Channel\"] == channel]\n",
    "            \n",
    "        # 2. Split df again by organelle and store in dict\n",
    "    #generate global variables which can be called outside this function and in other functions\n",
    "            global organelles # a list\n",
    "            global ref_org # a value of this list\n",
    "            organelles = ch_df[\"Organelles\"].unique()\n",
    "            org_df = {org: ch_df[ch_df[\"Organelles\"] == org] for org in organelles}\n",
    "        # 3. Start with the first organelle\n",
    "            ref_org = organelles[0]\n",
    "            for org in organelles:\n",
    "                if org == ref_org:\n",
    "            # Rename columns to add organelle\n",
    "                    organelle_base = (org_df[org]\n",
    "                                      .drop(columns=\"Organelles\")\n",
    "                                      .rename(columns=lambda col: f\"{col}_{org}\" if col not in [\"Cell_No\", \"Label\", \"Channel\"] else col)\n",
    "                    )\n",
    "        \n",
    "        # 4. Merge with all other dfs filtered by organelle\n",
    "                else:\n",
    "                    organelle_x = (org_df[org]\n",
    "                                   .drop(columns=\"Organelles\")\n",
    "                                   .rename(columns=lambda col: f\"{col}_{org}\" if col not in [\"Cell_No\", \"Label\", \"Channel\"] else col)\n",
    "                    )\n",
    "        # 5. Merge dfs on cells to keep the assignment of a punctum to its cell, tidy up\n",
    "                    organelle_base = (pd.merge(organelle_base, \n",
    "                                              organelle_x.drop(columns=[\"Label\"]), \n",
    "                                               on = [\"Cell_No\", \"Channel\"],\n",
    "                                               how=\"left\")\n",
    "                                      .drop(columns=[\"Label\", \"Channel\"])\n",
    "                                     )\n",
    "        \n",
    "                    ordered_files[f\"{name} {channel}\"] = organelle_base\n",
    "    return ordered_files\n",
    "\n",
    "def Calculate (filtered):\n",
    "    \"\"\"renames columns according to measured organelle, calculates Mean and Standard Error for each seperate file (field of view)\n",
    "    \"\"\"\n",
    "    files = filtered.copy()\n",
    "    calc_files = {}\n",
    "\n",
    "    for name, file in files.items():\n",
    "        summary = pd.DataFrame(columns=[\"\"])\n",
    "        summary.loc[0] = [None] #dummy row necessary to add new rows\n",
    "    #Calculate extra statistics\n",
    "        for org in organelles:\n",
    "            summary[f\"Mean: Area_{org}\"] = file[f\"Area_{org}\"].mean()\n",
    "            summary[f\"Mean: Px Int Mean_{org}\"] = file[f\"Px Int Mean_{org}\"].mean()\n",
    "            summary[f\"SEM: Px Int_{org}\"] = file[f\"Px Int Mean_{org}\"].sem()\n",
    "    #Add number of cells counted        \n",
    "        summary[\"# of cells\"] = file[\"Cell_No\"].nunique()     \n",
    "    #Split file into new dfs puncta and no-puncta\n",
    "        file_nopuncta = file[file[\"Px Int Mean_puncta\"].isna()]\n",
    "        file_puncta = file[file[\"Px Int Mean_puncta\"].notna()]\n",
    "    \n",
    "    #Store summarized results  \n",
    "        summary[\"all puncta\"] = file_puncta[\"Cell_No\"].count()\n",
    "        summary[\"puncta per cell\"] = summary[\"all puncta\"] / summary[\"# of cells\"]\n",
    "        summary[\"no puncta per cell\"] = file_nopuncta[\"Cell_No\"].count() / summary[\"# of cells\"]\n",
    "    \n",
    "    #Count how many times there are puncta per cell\n",
    "        puncta_counts = file_puncta[\"Cell_No\"].value_counts().value_counts().sort_index()\n",
    "    \n",
    "    #Add each count as a new column to summary df\n",
    "        for puncta_bin, puncta_count in puncta_counts.items():\n",
    "            col_name = f\"{puncta_bin} puncta per cell\"\n",
    "            summary[col_name] = puncta_count / summary[\"# of cells\"]\n",
    "\n",
    "        final = (file.join(summary.drop(columns=\"\"), how=\"outer\")\n",
    "             .set_index(\"Cell_No\")\n",
    "            )\n",
    "        calc_files[name] = final\n",
    "    return calc_files\n",
    "\n",
    "def Summarize (calc_files):\n",
    "    \"\"\"\n",
    "    Combine all single files generated earlier into two summaries of measurements & counted puncta\n",
    "    Input dfs contain all single cell outputs and a single row with all means and SEM\n",
    "    The function uses multiindexing to combine it, keep every single df and keep it tidy\n",
    "    \"\"\"\n",
    "    quant_files = copy.deepcopy(calc_files)\n",
    "    # match file name: Sil1-sfGFP DTT_001 Nsp1_column names\n",
    "    fileregex = r\"(?P<POI>^.*) (?P<cond>.+)_\\d* (?P<channel>[a-zA-Z0-9\\- ]+)_?(?P<col>.*)\" #'?' matches the previous thingy 0 or 1 times\n",
    "    summary = pd.DataFrame()\n",
    "            \n",
    "    for name, file in quant_files.items():\n",
    "        \n",
    "        for org in organelles:         \n",
    "            subset = [\"Cell_No\", f\"Area_{org}\", f\"Px Int Mean_{org}\"]\n",
    "            puncta_subset = ['# of cells','all puncta'] + [col for col in file.columns if 'per cell' in col] #concat the 2 lists\n",
    "        \n",
    "        cond = re.search(fileregex, name).group(\"cond\")\n",
    "        channel = re.search(fileregex, name).group(\"channel\")\n",
    "        file.columns = [f\"{cond} {channel}_{col}\" for col in file.columns] #rename all columns to generate the multiindex\n",
    "    \n",
    "        multiindex = pd.MultiIndex.from_arrays(\n",
    "            arrays=[[channel] * len(file.columns),\n",
    "                    [cond] * len(file.columns),\n",
    "                    # file columns look like: DTT channel 2_col_name\n",
    "                    [col.split(\"_\", 1)[1] for col in file.columns] #split once after the first \"_\"\n",
    "                   ], names=[\"Channel\", \"Condition\", \"Col\"]\n",
    "        )\n",
    "        file.columns = multiindex\n",
    "        #file_stacked = file.stack([\"Channel\", \"Condition\"], future_stack=True).droplevel(0) #drop \"Cell_No\"\n",
    "        file_stacked = file.stack([\"Channel\", \"Condition\"], future_stack=True).reset_index(level=\"Cell_No\") #put Cell_No back into columns\n",
    "        puncta_df = file_stacked[puncta_subset]\n",
    "         \n",
    "        if summary.empty:\n",
    "            #summary = file_stacked[subset].dropna()\n",
    "            summary = file_stacked[subset]\n",
    "            puncta_summary = puncta_df.dropna()\n",
    "        else:\n",
    "            #summary = pd.concat([summary, file_stacked[subset].dropna()], axis=0)\n",
    "            summary = pd.concat([summary, file_stacked[subset]], axis=0)\n",
    "            puncta_summary = pd.concat([puncta_summary, puncta_df.dropna()], axis=0)\n",
    "            \n",
    "        #subset the df, groupby condition and channel, calculate mean or sum, add a prefix to columns\n",
    "        summary_mean = summary.loc[:,[col for col in summary.columns]].groupby([\"Channel\",\"Condition\"]).mean().add_prefix(\"Mean: \")\n",
    "        puncta_mean = (puncta_summary\n",
    "                       .loc[:, [col for col in puncta_summary.columns if col not in [\"# of cells\", \"all puncta\"]]] #select all except the specified ones\n",
    "                       .groupby([\"Channel\",\"Condition\"]).mean().add_prefix(\"Mean: \")\n",
    "                      )\n",
    "        puncta_sum = puncta_summary.loc[:,[\"# of cells\", \"all puncta\"]].groupby([\"Channel\",\"Condition\"]).sum().add_prefix(\"Combined \")\n",
    "        \n",
    "        # Calculate puncta per cell + SEM\n",
    "        puncta_mean[\"puncta per cell\"] = puncta_sum[\"Combined all puncta\"] / puncta_sum[\"Combined # of cells\"]\n",
    "        puncta_mean[\"SEM: puncta per cell\"] = summary.groupby([\"Channel\", \"Condition\", \"Cell_No\"]).size().groupby([\"Channel\", \"Condition\"]).sem() #calculates group size -> puncta per cell and then again the sem for each group\n",
    "\n",
    "        # Calculate SEM\n",
    "        summary_sem = summary.loc[:, [col for col in summary.columns]].groupby([\"Channel\",\"Condition\"]).sem().add_prefix(\"SEM: \")                     \n",
    "                     \n",
    "        # Merge the mean and sem dfs\n",
    "        summary_merged = pd.concat([summary_mean, summary_sem], axis=1)\n",
    "        puncta_merged = pd.concat([puncta_mean, puncta_sum], axis=1)\n",
    "        \n",
    "    return summary_merged, puncta_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ababbe-a544-4605-8186-7da93a8cc374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ordered_files = Filter(rawfiles)\n",
    "quant_files = Calculate(ordered_files)\n",
    "summary, puncta = Summarize(quant_files)\n",
    "\n",
    "# Custom order for Condition & Channel\n",
    "cond_order = {'nt': 0, 'DTT': 1}\n",
    "chan_order = {'Nsp1':0, 'Nup116':1, 'Nup159':2, 'Nup84':3, 'Nup157':4, 'Pom152':5}\n",
    "\n",
    "puncta_order = (\n",
    "    puncta.reset_index()\n",
    "      .assign(cond_order=lambda x: x['Condition'].map(cond_order))\n",
    "      .assign(channel_order=lambda x: x['Channel'].map(chan_order))\n",
    "      .sort_values(['channel_order', 'cond_order'])\n",
    "      .drop(columns=['cond_order', 'channel_order'])\n",
    "      .set_index(['Channel', 'Condition'])\n",
    ")\n",
    "summary_order = (\n",
    "    summary.reset_index()\n",
    "      .assign(cond_order=lambda x: x['Condition'].map(cond_order))\n",
    "      .assign(channel_order=lambda x: x['Channel'].map(chan_order))\n",
    "      .sort_values(['channel_order', 'cond_order'])\n",
    "      .drop(columns=['cond_order', 'channel_order'])\n",
    "      .set_index(['Channel', 'Condition'])\n",
    ")\n",
    "\n",
    "puncta_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadabb05-8576-46eb-8057-53476147a0c6",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"generate an excel file with all single files as sheets and summary sheets and save\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "save_path = os.path.join(\n",
    "    os.path.dirname(file_dir),\n",
    "    f\"{os.path.basename(file_dir).split(\"_\",1)[0]}_{POI}\"\n",
    "    )+\".xlsx\"\n",
    "print(save_path)\n",
    "\n",
    "with pd.ExcelWriter(save_path, engine='openpyxl') as writer: #the file is automatically saved when the \"with\" block is finished\n",
    "    summary_order.to_excel(writer, sheet_name=\"CombinedSummary\", index=True)\n",
    "    puncta_order.to_excel(writer, sheet_name=\"Puncta Summary\", index=True)\n",
    "    #Save each individual merged_df to a combined excel file \n",
    "    for name, file in quant_files.items():\n",
    "        #match the first part of the name without the hyperstack and use as excel sheet name\n",
    "        sheet_name = name # change according to folder\n",
    "        file.to_excel(writer, sheet_name=sheet_name, index=True)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c77f02-5f19-4dc0-804b-4a9259327daa",
   "metadata": {},
   "source": [
    "# Plot the results: Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ba524-f543-4dc3-8ab3-6c150d8d4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plot bar graph to show distribution of puncta during different conditions and Nups\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_palette = ['#000000', '#e6e6e6', \n",
    "                 '#8601AF', '#0247FE', '#66B032', '#FCCC1A', '#FB9902', '#FE2712']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "histogram = input(\"Do you only want to plot puncta? -yes -no\")\n",
    "\n",
    "if histogram == \"yes\":\n",
    "    puncta_plot = puncta_order.copy().filter(items = [\"Mean: puncta per cell\"])\n",
    "    puncta_sem = puncta_order.copy().filter(items = [\"SEM: puncta per cell\"])\n",
    "    puncta_plot.columns = puncta_plot.columns.str.replace(\"Mean: \", \"\", regex=False)\n",
    "    puncta_sem.columns = puncta_sem.columns.str.replace(\"SEM: \", \"\", regex=False)\n",
    "\n",
    "    x = np.arange(len(puncta_plot))  # bar groups (12)\n",
    "    bar_width = 0.9\n",
    "    n_bars = len(puncta_plot.columns)\n",
    "    \n",
    "    # Plot bars with error bars and caps\n",
    "    for i, col in enumerate(puncta_plot.columns):\n",
    "        x_pos = x + i * bar_width / n_bars\n",
    "        ax.bar(\n",
    "            x_pos,\n",
    "            puncta_plot[col],\n",
    "            yerr=puncta_sem[col],\n",
    "            width=bar_width / n_bars,\n",
    "            label=col,\n",
    "            color=color_palette[i],\n",
    "            edgecolor='black',\n",
    "            capsize=5\n",
    "        )\n",
    "    \n",
    "    save_path = os.path.join(os.path.dirname(file_dir), f\"Puncta count\")\n",
    "else:\n",
    "    puncta_plot = puncta_order.copy().filter(items = [\"Mean: puncta per cell\",  \"Mean: no puncta per cell\", \n",
    "                                                      \"Mean: 1 puncta per cell\", \"Mean: 2 puncta per cell\", \"Mean: 3 puncta per cell\", \"Mean: 4 puncta per cell\", \"Mean: 5 puncta per cell\", \"Mean: 6 puncta per cell\"\n",
    "                                                     ])\n",
    "    puncta_plot.columns = puncta_plot.columns.str.replace(\"Mean: \", \"\", regex=False)\n",
    "    \n",
    "    puncta_plot.plot(kind='bar', ax=ax, width=0.9, color=color_palette, edgecolor='black')\n",
    "    \n",
    "    save_path = os.path.join(os.path.dirname(file_dir), f\"Puncta Histogram\")\n",
    "\n",
    "\n",
    "# Set axis titles\n",
    "ax.set_title(\"Frequency of cytosolic Nup puncta per cell\")\n",
    "ax.set_ylabel(\"Puncta Count per cell\", fontsize=12)\n",
    "ax.set_xlabel(\"\")\n",
    "\n",
    "# Generate stacked x-axis labels\n",
    "# Two-line xtick labels: Condition (bottom) + Channel (top)\n",
    "ax.set_xticklabels([f\"{cond}\" for chan, cond in puncta_plot.index], \n",
    "                   rotation=0, fontsize=10)\n",
    "ax.set_xticks(x -0.4 + bar_width / 2)\n",
    "ax.tick_params(axis='x', length=0)  # \"remove\" xticks\n",
    "\n",
    "# Add a top x-axis for 'Channel' (e.g., Nsp1)\n",
    "channels = [chan for chan, cond in puncta_plot.index]\n",
    "\n",
    "# Find where each channel starts and ends\n",
    "positions = np.arange(len(channels))\n",
    "channel_labels = []\n",
    "channel_centers = []\n",
    "for ch in dict.fromkeys(channels):  # preserves order & uniqueness\n",
    "    idx = [i for i, c in enumerate(channels) if c == ch]\n",
    "    channel_centers.append((sum(idx) / len(idx)))\n",
    "    channel_labels.append(ch)\n",
    "\n",
    "# Add secondary x-axis on top\n",
    "ax2 = ax.secondary_xaxis('bottom')\n",
    "ax2.set_xticks(channel_centers)\n",
    "ax2.set_xticklabels(channel_labels, fontsize=12)\n",
    "ax2.tick_params(axis='x', length=0, pad=20) #remove xticks, adjust label spacing\n",
    "\n",
    "\n",
    "# Legend outside\n",
    "ax.legend(\n",
    "    title='Mean counts per cell',\n",
    "    bbox_to_anchor=(1.01, 1),\n",
    "    loc='upper left',\n",
    "    borderaxespad=0.\n",
    ")\n",
    "\n",
    "# save the plots\n",
    "saveplots = input(\"Do you want to save the plots? -yes -no\")   \n",
    "\n",
    "if saveplots == \"yes\":\n",
    "    plt.savefig((save_path +\".svg\"), \n",
    "                format='svg', dpi=300, bbox_inches='tight')  # Save with high resolution, crop whitespace around\n",
    "    plt.savefig((save_path +\".png\"), \n",
    "                dpi=300, bbox_inches='tight')  # Save with high resolution, crop whitespace around\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
